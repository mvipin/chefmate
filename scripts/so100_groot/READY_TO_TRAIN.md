# ‚úÖ Multi-Task Training Ready!

## Pre-Flight Check Complete

### ‚úì Datasets Verified
- **Cheese**: 50 episodes at `~/Isaac-GR00T/demo_data/cheese`
- **Bread**: 50 episodes at `~/Isaac-GR00T/demo_data/bread`
- Both uploaded to Hugging Face Hub

### ‚úì Base Model Checkpoint
- **checkpoint-10000** (fully trained cheese model)
- Location: `~/so100-groot-checkpoints/cheese/checkpoint-10000`
- Loss: 0.025 (97% reduction from initial 0.873)

### ‚úì Training Configuration
```bash
Base Model:     checkpoint-10000 (cheese)
Datasets:       cheese + bread
Learning Rate:  0.00005 (50% reduced to prevent catastrophic forgetting)
Max Steps:      8000
Batch Size:     16
Grad Accum:     8 (effective batch = 128)
LoRA Rank:      32
Output Dir:     ~/so100-groot-checkpoints/cheese_bread_multitask
```

## Start Training

### Quick Start
```bash
cd ~/lerobot
./scripts/so100_groot/03_train_model.sh
```

### Monitor Training

**Terminal 1: GPU Usage**
```bash
watch -n 1 nvidia-smi
```

**Terminal 2: TensorBoard**
```bash
tensorboard --logdir ~/so100-groot-checkpoints/cheese_bread_multitask/tensorboard/
```

**Terminal 3: Training Progress**
```bash
tail -f ~/so100-groot-checkpoints/cheese_bread_multitask/train.log
```

## Expected Timeline

| Milestone | Steps | Time | Expected Loss |
|-----------|-------|------|---------------|
| Start | 0 | 0 min | ~0.025 (from checkpoint) |
| Early | 500 | ~15 min | ~0.020-0.022 |
| Mid | 2000 | ~1 hour | ~0.018-0.020 |
| Late | 4000 | ~2 hours | ~0.015-0.018 |
| Complete | 8000 | ~2.5-3.5 hours | ~0.012-0.015 |

## What to Watch For

### ‚úÖ Good Signs
- Loss decreases gradually
- No sudden spikes in loss
- GPU utilization stays at 90-100%
- VRAM usage stable at 12-14GB
- Regular checkpoint saves every 500 steps

### ‚ö†Ô∏è Warning Signs
- Loss increases (catastrophic forgetting)
  - **Fix**: Stop training, reduce learning rate to 0.000025
- Loss plateaus early (< 1000 steps)
  - **Fix**: Increase learning rate slightly to 0.000075
- OOM errors
  - **Fix**: Reduce batch size to 12, reduce workers to 4

## After Training

### 1. Evaluate Best Checkpoint
```bash
# Check final checkpoints
ls -lh ~/so100-groot-checkpoints/cheese_bread_multitask/

# Typical checkpoints:
# checkpoint-500, checkpoint-1000, ..., checkpoint-8000
```

### 2. Update Deployment Script
```bash
# Edit scripts/so100_groot/05b_deploy_robot_smooth.sh
# Change CHECKPOINT_PATH to:
CHECKPOINT_PATH="$HOME/so100-groot-checkpoints/cheese_bread_multitask/checkpoint-8000"
```

### 3. Test Both Tasks
- Test cheese pick-and-place
- Test bread pick-and-place
- Verify no performance degradation on cheese task
- Verify good performance on bread task

## Training Command

The script will execute:
```bash
python scripts/gr00t_finetune.py \
    --dataset-path ./demo_data/cheese/ ./demo_data/bread/ \
    --num-gpus 1 \
    --output-dir ~/so100-groot-checkpoints/cheese_bread_multitask \
    --max-steps 8000 \
    --data-config so100_dualcam \
    --video-backend torchvision_av \
    --batch-size 16 \
    --gradient-accumulation-steps 8 \
    --dataloader-num-workers 8 \
    --save-steps 500 \
    --learning-rate 0.00005 \
    --lora-rank 32 \
    --lora-alpha 64 \
    --lora-dropout 0.1 \
    --no-tune_diffusion_model \
    --balance-dataset-weights true \
    --balance-trajectory-weights true \
    --base-model-path ~/so100-groot-checkpoints/cheese/checkpoint-10000 \
    --report-to tensorboard
```

## Key Features

### LeRobotMixtureDataset Benefits
- ‚úÖ No physical dataset merging required
- ‚úÖ Automatic balanced sampling from both datasets
- ‚úÖ Weighted trajectory sampling
- ‚úÖ Native multi-task support in GR00T

### Catastrophic Forgetting Prevention
- ‚úÖ 50% reduced learning rate (0.00005 vs 0.0001)
- ‚úÖ Starting from fully trained checkpoint
- ‚úÖ Balanced dataset weights
- ‚úÖ Shorter training (8000 vs 10000 steps)

## Success Criteria

After training, the model should:
1. ‚úÖ Maintain cheese task performance (>90% success rate)
2. ‚úÖ Achieve bread task performance (>85% success rate)
3. ‚úÖ Show smooth motion for both tasks
4. ‚úÖ Final loss < 0.015

## Next Steps After Success

1. **Add More Ingredients**
   - Record lettuce dataset
   - Record tomato dataset
   - Continue multi-task training

2. **Full Sandwich Assembly**
   - Chain multiple tasks together
   - Test complete sandwich workflow
   - Optimize task sequencing

3. **Deployment Optimization**
   - Fine-tune action smoothing
   - Optimize action horizon
   - Test temporal ensembling

---

**Ready to start? Run:**
```bash
cd ~/lerobot && ./scripts/so100_groot/03_train_model.sh
```

Good luck! üöÄ

